# lab_10
### Проделанная работа
В этом ноутбуке были выполнены следующие шаги с использованием PySpark:

1.  **Загрузка данных:** Исходный набор данных "student-data-csv" был загружен с Kaggle с использованием библиотеки `kagglehub`.
2.  **Инициализация Spark:** Была создана сессия Spark (`SparkSession`) для работы с распределенными данными.
3.  **Чтение данных:** Данные из CSV-файла "student-data.csv" были прочитаны в PySpark DataFrame.
4.  **Исследование данных:**
    *   Отображены первые 5 строк DataFrame (`df.show(5)`).
    *   Выведена схема данных (`df.printSchema()`) для проверки типов данных.
    *   Выбраны и показаны определенные столбцы ("school", "age") (`df.select("school", "age").show()`).
    *   Отфильтрованы данные для строк, где возраст (`age`) больше 17 (`df.filter(df["age"] > 17).show()`).
    *   Отфильтрованы данные для строк, где возраст (`age`) меньше 21 (`df.where(df.age < 21).show()`).
    *   Данные отсортированы по убыванию количества пропусков (`absences`) (`df.orderBy(df["absences"].desc()).show()`).
    *   Получена описательная статистика для DataFrame (`df.describe().show()`).
5.  **Сохранение данных:** DataFrame был сохранен в формате CSV в папку "/content/output1_students" (`df.write.csv("/content/output1_students", header=True)`).

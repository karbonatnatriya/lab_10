# Анализ образовательных данных (Student Performance)

[![Python](https://img.shields.io/badge/Python-3%2B-blue)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5.0-red)](https://spark.apache.org/)
[![Pandas](https://img.shields.io/badge/Pandas-1.5%2B-orange)](https://pandas.pydata.org/)

## Содержание
1. [Краткое резюме](#краткое-резюме)
2. [Архитектура данных и ETL-процесс](#архитектура-данных-и-etl-процесс)
3. [Используемые технологии](#используемые-технологии)
4. [Структура проекта](#структура-проекта)
5. [Воспроизведение проекта](#воспроизведение-проекта)
6. [Датакаталог](#датакаталог)
7. [Описание данных](#описание-данных)

## Краткое резюме

**Цель:** Исследовать факторы, влияющие на академическую успеваемость студентов, с использованием инструментов Big Data.

**Задачи:**
- Загрузить и провести первичный анализ набора данных о студентах.
- Отработать основные операции с данными в PySpark: фильтрация, сортировка, агрегация.
- Построить воспроизводимый ETL-конвейер.
- Создать профессиональную документацию проекта.

**Ожидаемые результаты:**
- Аналитический отчет с ключевыми инсайтами о данных.
- Очищенный и готовый к использованию набор данных.
- Полная документация, включая датакаталог.

## Архитектура данных и ETL-процесс

Процесс обработки данных следует упрощенному ETL-паттерну:

1. **Extract (Извлечение):** Исходные данные в формате CSV загружаются из папки `data/raw/` с помощью PySpark.
2. **Transform (Трансформация):**
   - Проводится первичный осмотр данных (`show`, `printSchema`).
   - Выполняются операции по фильтрации, селекции и сортировке.
   - (На последующих этапах) Может быть добавлена очистка от пропусков, обработка выбросов, feature engineering.
3. **Load (Загрузка):** Результаты обработки сохраняются в папку `data/processed/` для последующего анализа.

## Используемые технологии

- **Язык программирования:** Python 3+
- **Обработка данных:** PySpark, Pandas
- **Хранение данных:** CSV, SQLite (может быть добавлена)
- **Ноутбуки:** Jupyter
- **Система контроля версий:** Git

## Структура проекта

```
Lab_12/
│
├── README.md                       # Этот файл
├── requirements.txt                # Зависимости Python
├── data_catalog.csv                # Динамический каталог данных
│
├── data/
│   ├── raw/                        # Исходные, неизменные данные
│   │   └── student-data.csv
│   └── processed/                  # Финальные данные
│       └── output1_students/       # Результаты обработки PySpark
│
├── notebooks/                      # Исследовательский анализ
│   └── ab_10.ipynb                 # Исходный ноутбук с анализом
│
│
└── output/                         # Графики, отчеты, презентации
```

## Воспроизведение проекта

### 1. Клонирование и настройка окружения

```bash
# Пример (если проект в Git)
git clone <url-вашего-репозитория>
cd ab_12

# Создание виртуального окружения (рекомендуется)
python -m venv venv
source venv/bin/activate  # Для Windows: venv\Scripts\activate

# Установка зависимостей
pip install -r requirements.txt
```

*Содержимое `requirements.txt`:*
```txt
pyspark>=3.5.0
pandas>=1.5.0
jupyter
kagglehub
```

### 2. Запуск ETL-пайплайна

Основная логика находится в ноутбуке `notebooks/ab_10.ipynb`. Для запуска выполните:

```bash
jupyter notebook
```

и откройте указанный ноутбук, выполнив все ячейки.

*В будущем логику из ноутбука следует перенести в `scripts/etl_pipeline.py` для автоматизации.*

### 3. Генерация датакаталога

После выполнения ETL-процесса обновите датакаталог:

```bash
python scripts/create_data_catalog.py
```

## Датакаталог

Этот проект использует **динамический датакаталог** для обеспечения прозрачности и актуальности метаданных.

- **Файл каталога:** [`data_catalog.csv`](data_catalog.csv)
- **Скрипт обновления:** [`scripts/create_data_catalog.py`](scripts/create_data_catalog.py)

Каталог автоматически собирает информацию о всех файлах данных в проекте, включая:
- Имя и путь к файлу.
- Размер и дату изменения.
- Количество столбцов и их имена.
- Стадию обработки данных (raw/interim/processed).
- Описание и источник набора данных.

**Для просмотра актуального состояния данных** откройте файл `data_catalog.csv` или запустите скрипт генерации заново.

## Описание данных

### Ключевые датасеты

| Имя файла | Стадия | Описание | Примеры полей |
| :--- | :--- | :--- | :--- |
| `student-data.csv` | `data/raw` | Исходные данные о студентах. | `school (string)`, `age (int)`, `absences (int)`, `G1/G2/G3 (int)` |
| `output1_students/` | `data/processed` | Результат обработки в PySpark. | Соответствует исходным данным, отфильтрованным и отсортированным. |

*Полное и актуальное описание всех датасетов смотрите в [`data_catalog.csv`](data_catalog.csv).*
